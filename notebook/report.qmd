---
title: "MY472 Summative 2 Project"
author: "Candidate 30552"
format: 
    html:
        embed-resources: true 
---


```{r, include = FALSE} 
knitr::opts_chunk$set(
    echo = FALSE, 
    eval = FALSE,
    message = FALSE, 
    warning = FALSE)

```

```{r basic-setup, eval = TRUE}

# Define function to install or load packages
load_packages <- function(x) {
  y <- x %in% rownames(installed.packages())
  if(any(!y)) install.packages(x[!y])
  invisible(lapply(x, library, character.only=T))
  rm(x, y)
}

# Load required packagess
load_packages(c(
    "tidyverse",
    "here",
    # Database management
    "DBI",
    "RSQLite",
    # APIs and webscraping
    "httr",
    "RSelenium",
    # Text analysis 
    "tm", 
    # Geospatial plots
    "tmap", 
    "sf", 
    "ggrepel", 
    # Random forests
    "parallel",
    "ranger",
    "tidymodels",
    "vip", 
    "rpart", 
    "rpart.plot"
    ))

db <- DBI::dbConnect(RSQLite::SQLite(), here("data/parliament_database.sqlite"))


```



```{r functions, eval = TRUE}

db_table_check <- function(database, table){
  
  rows <- dbGetQuery(database, paste0("SELECT COUNT(1) FROM ", table))
  cols <- dbListFields(database, table)
  cols_n <- length(cols)
  
  result = list(
    n_rows = rows[[1]],
    n_cols = cols_n,
    col_names = cols)

  return(result)
}

# Define function to print loop progress 

progress_perc <- function(num_done, total, additional_message = ""){
    cat("\r", paste0(additional_message, " ", round(num_done/total*100), "% "))
}

replace_null_with_na <- function(x) {
  if (is.list(x)) { # Checks for whether the item is a sublist 
    lapply(x, replace_null_with_na) # if it is, apply the function for each of the elements within the sublist
  } else { # If it isn't, simply apply the main function
    ifelse(is.null(x) || x == "null", "NA", x) 
  }
}


```


# Introduciton 

# Data 



To store the data I collect, I will use a local relational database. This will improve the efficiency of the storage because... It will also allow me to query only the variables I need for anlausis 

I draw on 4 endpoint from parlimanet's API to construct four three tables that I write out to database.

1. Oral questions 
2. Members
3. Constituencies 
4. Elections

Tables: 

1. oral_questions
2. mps
3. constituencies (containing election results)

I then will create an additional table with the results of the measurmeent I use to classify questions. 

4. question_topics 


## Parliament API 

First, I scraped all questions availble from the written and oral endpoints. Both endpoints only return up to 100 questions with one request. However, you can skip responses. Therefore, you can retrieve all questions by looping through... 

These reponses do not return the full text of each question, so I use these responses as a sampling frame 

```{r pull-oral-questions}

# This code can also be run by sourcing scripts/01_pull-oral-questions.R 

GET_qs <- function(endpoint_url, n_skip = 0) {
  url <- paste0(
    endpoint_url,
    "?parameters.skip=",
    n_skip,
    "&parameters.answeringDateStart=2023-01-01&parameters.answeringDateEnd=2023-12-31", # Limit to 2023
    "&parameters.take=100")

  response <-
    httr::GET(url) %>%
    httr::content("parsed") # Use :: because tm masks content 

  return(response)
}

# Define functions to pull all questions

pull_all_oral_qs <- function(endpoint_url){

  # Calculate how many questions are in the end point
  n_resp <- httr::GET(paste0(
    endpoint_url,
    "?parameters.answeringDateStart=2023-01-01&parameters.answeringDateEnd=2023-12-31", # Limit to 2023
    "&parameters.take=1")) %>%
    httr::content("parsed")
  n <- n_resp$PagingInfo$GlobalTotal

  # Questions can be pulled in batches of 100,
  # calculate how many time we will have to pull
  n_loops <- ceiling(n / 100)

  print(paste0("LOG | ", Sys.time(), " | Oral question pull starting"))

  for (i in 1:n_loops) {

    n_skip <- (i - 1) * 100 # Skip however many 100s the loop has run

    if (i == 1) { # On first iteration, make new list

      response <- GET_qs(endpoint_url, n_skip)
      response <- response$Response

    } else { # On all other iterations, append to existing list

      response_new <- GET_qs(endpoint_url, n_skip)
      response_new <- response_new$Response
      response <- c(response, response_new) # Merge responses

    }

    print(paste0("LOG | ", Sys.time(), " | ", i, " of ", n_loops, " done.")) # Print progress message
    Sys.sleep(1) # Sleep to avoid hammering the API

  }

  print(paste0("LOG | ", Sys.time(), " | Oral question pull done :)"))
  return(response)
}

## APPLY FUNCTIONS

oral_questions <- pull_all_oral_qs(
  "https://oralquestionsandmotions-api.parliament.uk/oralquestions/list")

saveRDS(oral_questions, "data/oral_questions_2023.RDS")

```


```{r clean-oral-questions, eval = TRUE}

oral_questions <- readRDS("data/oral_questions_2023.RDS")


for (i in seq_along(oral_questions)) { 
   # remove sublists, otherwise names do not match
    oral_questions[[i]]$AskingMember <- NULL
    oral_questions[[i]][["AnsweringMinister"]] <- NULL

  if (i == 1){
    question_df <- data.frame(lapply(
      oral_questions[[i]],
      function(x) ifelse(is.null(x), NA, x)))
  } else {
    question_df2 <- data.frame(lapply(
      oral_questions[[i]],
      function(x) ifelse(is.null(x), NA, x)))

    question_df <- rbind(question_df, question_df2)
  }

 progress_perc(i, length(oral_questions), "QUESTIONS:")

}

rm(question_df2, i)

### Clean dataframes and merge into one table ####

question_df <- question_df %>%
  select(
    question_id = Id,
    question_text = QuestionText,
    asking_member = AskingMemberId,
    question_tabled_when = TabledWhen,
    question_answering_when = AnsweringWhen,
    question_answering_body = AnsweringBody,
    question_answering_body_id = AnsweringBodyId,
    answering_member = AnsweringMinisterId)

```

```{r}
dbWriteTable(db, "oral_questions", question_df, overwrite = TRUE)
```

```{r}
db_table_check(db, "oral_questions")
```

Many MP characteristics change over time - such as the party they represent, the ministerial posts they hold, etc. Therefore, we have to construct a members table that accomodates these changes. The UK Parliament members API allows queries that specifiy a date. This will give us a unique response that is valid on the day of each question. I will write out this "summarised" version of the data to my database. Then in analysis, I can query the entry that is valid for when each question is asked 

```{r pull-members-endpoint}

# This code can also be run by sourcing scripts/02_pull-oral-questions.R 

pull_members <- function(base_url, df) {

  for (i in seq_along(df$member_id)) {

    url <- paste0( # Build request URL
      base_url, "/",
      df$member_id[i],
      "?detailsForDate=",
      df$question_tabled_when[i])

    if (i == 1) { # If 1st iteration, create response,

      response <- httr::GET(url) %>% httr::content("parsed") # Pull request
      response <- response[1] # Extract list with response
      response <- c(
        date = df$question_tabled_when[i], response[[1]]) # Merge with date
      response <- list(response) # Convert to list

    } else { #  else create response2, then merge
      response_new <- httr::GET(url) %>% httr::content("parsed")
      response_new <- response_new[1]
      response_new <- c(
        date = df$question_tabled_when[i], response_new[[1]])
      response_new <- list(response_new)

      response <- c(response, response_new) # Merge responses
    }

    Sys.sleep(1)

    print(paste0("LOG Member Pull | ", Sys.time(), " | ", i, " of ", nrow(df), " done"))
  }
  return(response)
}

# Query question table to get MP-date pairs

members_asking <- dbGetQuery(db,
  "
  SELECT 
    asking_member AS member_id, 
    question_tabled_when
  FROM oral_questions
  ")

ministers_answering <- dbGetQuery(db,
  "
  SELECT 
    answering_member AS member_id, 
    question_tabled_when
  FROM oral_questions
  ")

q_parameters <- rbind(members_asking, ministers_answering)

# Only keep unique MP-date pairs to avoid pulling the same information twice
q_parameters <- unique(q_parameters)


q_parameters <- q_parameters %>%
  mutate( # Change format of dates to just YYYY-MM-DD
    question_tabled_when = str_extract(question_tabled_when, ".+?(?=T)"))

# Apply function to pull members 
members <- pull_members(
  "https://members-api.parliament.uk/api/Members",
  q_parameters)

saveRDS(members, "data/members_raw.Rds")
```

```{r clean-members-pull}
members <- readRDS("data/members_raw.Rds")

# TODO run in quarto 

# Replace "null" values with NA so they are kept in the structure of the list
members <-  lapply(
  members, 
  function(x) {lapply(x, replace_null_with_na)})

for (i in seq_along(members)) {

  if (i == 1) {

    members_df <- members[i] %>%
      unlist() %>%
      t() %>%
      data.frame()

  } else {
    members_df_new <- members[i] %>%
      unlist() %>%
      t() %>%
      data.frame()

    members_df <- rbind(members_df, members_df_new)
  }
}

members_df <- members_df %>%
  select(
    member_date_valid = date, 
    member_id = id, 
    name_display = nameDisplayAs, 
    latest_party = latestParty.id,
    gender = gender, 
    latest_constituency = latestHouseMembership.membershipFromId,
    membership_start_date = latestHouseMembership.membershipStartDate, 
    membership_end_date = latestHouseMembership.membershipEndDate,
    membership_end_reason = latestHouseMembership.membershipEndReason
  )

# TODO add positions here? 

members_df <- members_df %>%
  group_by( # Group by all variables apart from date
    member_Mnis_ID,
    member_name_display,
    member_latest_party,
    member_gender,
    member_latest_constituency,
    member_membership_start_date,
    member_membership_end_date,
    member_membership_end_reason,
    member_name_full,
    member_name_list
  ) %>%
  summarize( # Summarise earliest date this is valid for and latest. This gives us a range of vlaues where this combination is duplicated 
    member_date_valid_min = min(member_date_valid), 
    member_date_valid_max = max(member_date_valid)
  )
```

```{r}
dbWriteTable(db, "members", members_df, overwrite = TRUE)
```


```{r}

db_table_check(db, "members")

```

```{r pull-constituency-endpoints}

MPs <- dbGetQuery(db,
  "
  SELECT * 
  FROM members 
  ")

constituencies <- MPs$member_latest_constituency %>% 
  unique()

constituencies <- 
  data.frame(
    constituency_id = constituencies
  ) %>%
  mutate(

    cons_name = NA, 
    cons_start_date = NA, 
    cons_end_date = NA,

    last_election_1_electorate = NA,
    last_election_1_turnout = NA,
    last_election_1_majority = NA, 
    last_election_1_result = NA, 
    last_election_1_winning_party = NA,
    last_election_1_election_ID = NA,
    last_election_1_electionDate = NA,
    last_election_1_isGeneralElection = NA,

    last_election_2_electorate = NA,
    last_election_2_turnout = NA,
    last_election_2_majority = NA, 
    last_election_2_result = NA, 
    last_election_2_winning_party = NA,
    last_election_2_election_ID = NA,
    last_election_2_electionDate = NA,
    last_election_2_isGeneralElection = NA,

    last_election_3_electorate = NA,
    last_election_3_turnout = NA,
    last_election_3_majority = NA, 
    last_election_3_result = NA, 
    last_election_3_winning_party = NA,
    last_election_3_election_ID = NA,
    last_election_3_electionDate = NA,
    last_election_3_isGeneralElection = NA,

    last_election_4_electorate = NA,
    last_election_4_turnout = NA,
    last_election_4_majority = NA, 
    last_election_4_result = NA, 
    last_election_4_winning_party = NA,
    last_election_4_election_ID = NA,
    last_election_4_electionDate = NA,
    last_election_4_isGeneralElection = NA,

    shapefile = NA
    )


### Basic details 

pull_const_info <- function(cons_id) {
  url <- paste0(
    "https://members-api.parliament.uk/api/Location/Constituency/",
    cons_id)

    basic_info <- httr::GET(url) %>%
    httr::content("parsed")

    return(basic_info)
}

# TODO remove progress bar
pb <- txtProgressBar(min = 0, max = length(constituencies$constituency_id), style = 3)


print(paste0(Sys.time(), " | BASIC INFO ..."))
cat("\n")

for(i in seq_along(constituencies$constituency_id)) {
  response <- pull_const_info(constituencies$constituency_id[i])
  response <- response[[1]]

  constituencies$cons_name[i] <- response$name
  constituencies$cons_start_date[i] <- response$startDate
  constituencies$cons_end_date[i] <- ifelse(is.null(response$endDate), NA, response$endDate)

  Sys.sleep(0.5)
  setTxtProgressBar(pb, i)
}


#saveRDS(constituencies, "data/constituencies_raw_basic.Rds")

print(paste0(Sys.time(), " | BASIC INFO done."))
cat("\n")

### Shape file

# Define function

get_cons_shapefile <- function(cons_id) {
  url <- paste0(
    "https://members-api.parliament.uk/api/Location/Constituency/",
    cons_id,
    "/Geometry")

  shapefile <- httr::GET(url) %>%
    httr::content("parsed")

  return(shapefile)
}

# Execute function
pb <- txtProgressBar(min = 0, max = length(constituencies$constituency_id), style = 3)


print(paste0(Sys.time(), " | SHAPE FILES ..."))
cat("\n")

for(i in seq_along(constituencies$constituency_id)) {
  response <- get_cons_shapefile(constituencies$constituency_id[i])
  response <- response[[1]]

  constituencies$shapefile[i] <- response

  Sys.sleep(0.5)
  setTxtProgressBar(pb, i)
}

saveRDS(constituencies, "data/constituencies_raw_shapefiles.Rds")

print(paste0(Sys.time(), " | SHAPE FILES done."))
cat("\n")

### Election results


get_cons_election_results <- function(cons_id) {
  url <- paste0(
    "https://members-api.parliament.uk/api/Location/Constituency/",
    cons_id,
    "/ElectionResults")

  results <- httr::GET(url) %>%
    httr::content("parsed")

  return(results)
}


print(paste0(Sys.time(), " | ELECTIONS ..."))
cat("\n")

for (i in seq_along(constituencies$constituency_id)) {
  response <- get_cons_election_results(constituencies$constituency_id[i])
  response <- response[[1]]

  response <- lapply(response, function(lst) {lapply(lst, replace_null_with_na)})

  constituencies$last_election_1_electorate[i] <- response[[1]]$electorate
  constituencies$last_election_1_turnout[i] <- response[[1]]$turnout  
  constituencies$last_election_1_majority[i] <- response[[1]]$majority
  constituencies$last_election_1_result[i] <- response[[1]]$result
  # If no winner recorded, skip this and assign NA
  if(length(response[[1]]$winningParty) > 1) { # When there is content in the winning party sublist, the length will be greater than 1
    constituencies$last_election_1_winning_party[i] <- response[[1]]$winningParty$id
  } else {
     constituencies$last_election_1_winning_party[i] <- NA
  }
  constituencies$last_election_1_election_ID[i] = response[[1]]$electionId
  constituencies$last_election_1_electionDate[i] = response[[1]]$electionDate
  constituencies$last_election_1_isGeneralElection[i] = response[[1]]$isGeneralElection

  constituencies$last_election_2_electorate[i] <- response[[2]]$electorate
  constituencies$last_election_2_turnout[i] <- response[[2]]$turnout 
  constituencies$last_election_2_majority[i] <- response[[2]]$majority
  constituencies$last_election_2_result[i] <- response[[2]]$result
  # If no winner recorded, skip this and assign NA
  if(length(response[[2]]$winningParty) > 1) {
    constituencies$last_election_2_winning_party[i] <- response[[2]]$winningParty$id
  } else {
     constituencies$last_election_2_winning_party[i] <- NA
  }
  constituencies$last_election_2_election_ID[i] = response[[2]]$electionId
  constituencies$last_election_2_electionDate[i] = response[[2]]$electionDate
  constituencies$last_election_2_isGeneralElection[i] = response[[2]]$isGeneralElection

  constituencies$last_election_3_electorate[i] <- response[[3]]$electorate
  constituencies$last_election_3_turnout[i] <- response[[3]]$turnout  
  constituencies$last_election_3_majority[i] <- response[[3]]$majority
  constituencies$last_election_3_result[i] <- response[[3]]$result
  # If no winner recorded, skip this and assign NA
  if(length(response[[3]]$winningParty) > 1) {
    constituencies$last_election_3_winning_party[i] <- response[[3]]$winningParty$id
  } else {
     constituencies$last_election_3_winning_party[i] <- NA
  }
  constituencies$last_election_3_election_ID[i] = response[[3]]$electionId
  constituencies$last_election_3_electionDate[i] = response[[3]]$electionDate
  constituencies$last_election_3_isGeneralElection[i] = response[[3]]$isGeneralElection

  constituencies$last_election_4_electorate[i] <- response[[4]]$electorate
  constituencies$last_election_4_turnout[i] <- response[[4]]$turnout   
  constituencies$last_election_4_majority[i] <- response[[4]]$majority
  constituencies$last_election_4_result[i] <- response[[4]]$result
  # If no winner recorded, skip this and assign NA
  if(length(response[[4]]$winningParty) > 1) {
    constituencies$last_election_4_winning_party[i] <- response[[4]]$winningParty$id
  } else {
     constituencies$last_election_4_winning_party[i] <- NA
  }
  constituencies$last_election_4_election_ID[i] = response[[4]]$electionId
  constituencies$last_election_4_electionDate[i] = response[[4]]$electionDate
  constituencies$last_election_4_isGeneralElection[i] = response[[4]]$isGeneralElection

  Sys.sleep(0.5)
  
  print(paste0(i, " of ", nrow(constituencies), " done."))
}

saveRDS(constituencies, "data/constituencies_raw.Rds")

```

- Constituency characteristics
  - Previous electoral results 
  - Socio-demographic features of the constituency


Constituency info
- We will get the cosntituency ID from the members pull. We can pull:
  - Election results for each constituency over different elections 
  - Shape files (for plotting later)
- Would like to add 
  - Basic demographic information 
  - Economic indicators 
  - Health indicators 




I obtained more constituency information from the UK House of Commons Library Constituecy dashboard. This does not have an API, and contains dynamic elements, so I use selinum to scrape it. 

This process was somewhat complicated by the fact that the dashboard is contained within an "iframe". This allows a different html tree to be embedded within the main html of the webpage meaning any CSS paths do not point to the actual path of the webpage. Do do this, we need to identify the iframe and use `switchToFrame()` to identify elements on the dashboard. 

HOUSEPRICE INFO ONLY IN ENG AND WALES, that's why so many NAs

```{r selenium-scrape-hoc-dashboard}

# This code can also be run by sourcing scripts/04_selinium-scrape-HoC-dashboard.R 

# Read in data from the constituency endpoint pull
cons <- readRDS("data/constituencies_raw.Rds") 

# Make new dataframe
cons <- cons %>%
  select(constituency_id, cons_name) %>%
  unique() %>% # Keep only unqiue constituencies 
  mutate( # Initialise variables
    region_nation_hoclib23 = NA,
    population_hoclib23 = NA,
    area_hoclib23 = NA,
    age_0_29_hoclib23 = NA,
    age_30_64_hoclib23 = NA,
    age_65_plus_hoclib23 = NA, 
    uc_claimants_hoclib23 = NA, 
    median_house_price_hoclib23 = NA
  )

# Set selinium browser
rD <- rsDriver(browser=c("firefox"), verbose = F, port = netstat::free_port(random = TRUE), chromever = NULL) 
driver <- rD$client

# Define a list of css selectors

selector_list <- list()

selector_list$search_dropdown <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[1]/transform/div/div[3]/div/div/visual-modern/div/div/div[2]/div/i"

selector_list$search_box <- "/html/body/div[7]/div[1]/div/div[1]/input"

selector_list$search_result <- "/html/body/div[7]/div[1]/div/div[2]/div/div[1]/div/div/div[1]/div/span"

selector_list$region_nation <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[2]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div"

selector_list$population <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[3]/transform/div/div[3]/div/div/visual-modern/div/div/div/p[2]/span"

selector_list$area <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[5]/transform/div/div[3]/div/div/visual-modern/div/div/div/p[2]/span"

selector_list$age_0_29 <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[11]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div/div[1]"

selector_list$age_30_64 <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[13]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div/div[1]"

selector_list$age_65_plus <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[15]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div/div[1]"

selector_list$uc_claimants <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[28]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div[1]/div[1]"

selector_list$house_prices <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[39]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div[2]/div[1]"

constituency_dash_scraper <- function(
  constituency_name, 
  wait_base = 1 # Allows user to adjust wait lengths (e.g if running on a slow connection)
                # If you get a 'could not find element' error, try adjusting the wait time as the dashboard takes a while to load 
  ){
  # Find dropdown box and click on it 
  search_dropdown <- driver$findElement(using = "xpath", value = selector_list$search_dropdown)
  search_dropdown$clickElement()
  
  # Find search box and type constituency name
  Sys.sleep(wait_base * 2)
  search_box <- driver$findElement(using = "xpath", value = selector_list$search_box)
  #search_box$clickElement() # Do not strictly need this, but if not working try uncommenting
  search_box$clearElement()
  search_box$sendKeysToElement(list(constituency_name))

  Sys.sleep(wait_base * 4) # This requires a long time to load.
  # Click on the first result to load data
  first_result <- driver$findElement(using = "xpath", value = selector_list$search_result)
  first_result$clickElement()
  
  Sys.sleep(wait_base * 4) # Wait for data to load
  
  # EXTRACT TEXT FROM ELEMENTS

  # Set defaults as NA
  region_nation_text <- NA
  population_text <- NA
  area_text <- NA
  age_0_29_text <- NA
  age_30_64_text <- NA
  age_65_plus_text <- NA
  uc_claimants_text <- NA
  house_prices_text <- NA

  # Region or nation
  tryCatch({ # Prevent loop from closing if no data available
  suppressMessages({ 
    region_nation <- driver$findElement(using = "xpath", value = selector_list$region_nation)
    region_nation_text <- region_nation$getElementText()[[1]]
    })
  }, error = function(e) {
    # Print error message, no need to assign NA as we have set NA as default
    print(paste0("Log: NA assigned for REGION/NATION at iteration ", i))
  })

  # Population 
  tryCatch({
  suppressMessages({ 
    population <- driver$findElement(using = "xpath", value = selector_list$population)
    population_text <- population$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for POPULATION at iteration ", i))
  })

  # Area in sq km
  tryCatch({
  suppressMessages({ 
    area <- driver$findElement(using = "xpath", value = selector_list$area)
    area_text <- area$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for AREA at iteration ", i))
  })

  # Age composition 
  tryCatch({
  suppressMessages({
    age_0_29 <- driver$findElement(using = "xpath", value = selector_list$age_0_29)
    age_0_29_text <- age_0_29$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for AGE 0-29 PLUS at iteration ", i))
  })

  tryCatch({
  suppressMessages({
    age_30_64 <- driver$findElement(using = "xpath", value = selector_list$age_30_64)
    age_30_64_text <- age_30_64$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for AGE 30-64 PLUS at iteration ", i))
  })

  tryCatch({
  suppressMessages({
    age_65_plus <- driver$findElement(using = "xpath", value = selector_list$age_65_plus)
    age_65_plus_text <- age_65_plus$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for AGE 64 PLUS at iteration ", i))
  })

  # Universal credit claimants 
  tryCatch({
  suppressMessages({
    uc_claimants <- driver$findElement(using = "xpath", value = selector_list$uc_claimants)
    uc_claimants_text <- uc_claimants$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for UC CLAIMANTS at iteration ", i))
  })

  # House price
  tryCatch({
    suppressMessages({
      house_prices <- driver$findElement(using = "xpath", value = selector_list$house_prices)
      house_prices_text <- house_prices$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for HOUSE PRICE at iteration ", i))
  })

 # Merge results into a list
  results = list(
    region_nation_text, 
    population_text, area_text, 
    age_0_29_text, age_30_64_text, age_65_plus_text,
    uc_claimants_text, house_prices_text)

  return(results)

}

# Run the scraper

# Navigate to home page outside of the loop to avoid reloading each time
driver$navigate("https://commonslibrary.parliament.uk/constituency-dashboard/")

Sys.sleep(1)

# The dashboard exists within a sub-page. Unless we "switch" to this subframe, the css paths will be broken
# Identify and switch to sub-page 
iframe <- driver$findElement(using = "xpath", value = "//iframe[@title='Constituency dashboard']")
driver$switchToFrame(iframe)
Sys.sleep(4)

# Set the number to start from in case loop is interuppted but we have cached results
start_from = 1 

for (i in start_from:length(cons$constituency_id)) {

  results <- constituency_dash_scraper(cons$cons_name[i], wait_base = 1)

    cons$region_nation_hoclib23[i] <- results[[1]]

    cons$population_hoclib23[i] <- results[[2]]

    cons$area_hoclib23[i] <- results[[3]]

    cons$age_0_29_hoclib23[i] <- results[[4]]
    cons$age_30_64_hoclib23[i] <- results[[5]]
    cons$age_65_plus_hoclib23[i] <- results[[6]]

    cons$uc_claimants_hoclib23[i] <- results[[7]]
    cons$median_house_price_hoclib23[i] <- results[[8]]

 # Cache results collected so far
  if(i == start_from){
    saveRDS(cons, paste0("data/cache_cons_at", i, ".Rds"))
  } else {
     saveRDS(cons, paste0("data/cache_cons_at", i, ".Rds"))
     file.remove(paste0("data/cache_cons_at", i-1, ".Rds")) # delete last cached object
  }
  
  Sys.sleep(1)

  print(paste0(i, " of ", nrow(cons), " done."))

}

# Kill driver and java processes
driver$close()
rD$server$stop()
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)

# Save output
saveRDS(cons, "data/hoc_library_scrape_raw.Rds")
```

```{r}
# Clean dashboard data

cons_hoc <- readRDS("data/hoc_library_scrape_raw.Rds")

# pop numeric 
cons_hoc$population_hoclib23 <- cons_hoc$population_hoclib23 %>%
  str_remove_all(",") %>%
  as.numeric()

# area numeric

cons_hoc$area_hoclib23 <- cons_hoc$area_hoclib23 %>%
  str_extract(".*(?=\\s*sq\\.\\s*km)") %>%
  str_remove_all(",") %>%
  as.numeric()


# age perc

cons_hoc$age_0_29_hoclib23 <- cons_hoc$age_0_29_hoclib23 %>%
  str_remove_all("%") %>%
  as.numeric()
cons_hoc$age_0_29_hoclib23 <- cons_hoc$age_0_29_hoclib23/100 # Convert to proportion 

cons_hoc$age_30_64_hoclib23 <- cons_hoc$age_30_64_hoclib23 %>%
  str_remove_all("%") %>%
  as.numeric()
cons_hoc$age_30_64_hoclib23 <- cons_hoc$age_30_64_hoclib23/100 # Convert to proportion 

cons_hoc$age_65_plus_hoclib23 <- cons_hoc$age_65_plus_hoclib23 %>%
  str_remove_all("%") %>%
  as.numeric()
cons_hoc$age_65_plus_hoclib23 <- cons_hoc$age_65_plus_hoclib23/100 # Convert to proportion 

# uc numeric

cons_hoc$uc_claimants_hoclib23 <- cons_hoc$uc_claimants_hoclib23 %>%
  str_remove_all(",") %>%
  as.numeric()

# house price numeric

cons_hoc$median_house_price_hoclib23 <- cons_hoc$median_house_price_hoclib23 %>%
  str_remove_all(",|Â£") %>%
  as.numeric()

```

```{r}

# read in data pulled from Parliament API 

cons_api <- readRDS("data/constituencies_raw.Rds")

# Merge 


cons_hoc <- cons_hoc %>%
  select(-cons_name)

cons <- left_join(cons_api, cons_hoc, by = "constituency_id")


# Write out to database 

dbWriteTable(db, "constituencies", cons, overwrite = TRUE)
```


```{r}
dbListTables(db)
db_table_check(db, "oral_questions")
db_table_check(db, "members")
db_table_check(db, "constituencies")
```

```{r, eval = TRUE}
# Disconnect from local database
DBI::dbDisconnect(db)
```


# Analysis 

## Measure topics 

```{r}
question_text <- dbGetQuery(
  db, 
  "
  SELECT question_id, question_text
  FROM oral_questions
  "
) 

# Measure 

# Initalise variables as FALSE
question_text$is_econ <- 0
question_text$is_health_welf <- 0

# clean question text 

question_text$question_text <- question_text$question_text %>%
    tolower() %>% # Convert to lower case
    tm::removePunctuation() # remove punctuation

# Define dictionaries 
# TODO: better define dictionaries
econ_dict <- 
  "econ*|inflat*|grow*|interest rate*|budget*|gdp|autumn statement*|financ*|chancellor"
health_welf_dict <- " nhs |universal credit| covid |pandemic*"

question_text <- question_text %>%
  mutate(
    is_econ = as.numeric(str_detect(question_text, econ_dict)), 
    is_health_welf = as.numeric(str_detect(question_text, health_welf_dict))
  )

 ``` 

```{r}

# Write out measured data to new table

question_text <- question_text %>% select(-question_text)

dbWriteTable(db, "question_topics", question_text, overwrite=TRUE)

```

## Descriptive analysis 

### Time 

```{r time-plot, eval = TRUE}

# Query database to return a table with the count of questions asked on each day 

time_data <- dbGetQuery(db, 
  "
  SELECT 
    oral_questions.question_tabled_when AS date_asked,
    SUM(question_topics.is_econ) AS econ_count,
    SUM(question_topics.is_health_welf) AS health_welf_count, 
    COUNT(*) AS total_question_count,
    SUM(question_topics.is_econ)/COUNT(*) AS econ_prop,
    SUM(question_topics.is_health_welf)/COUNT(*) AS health_welf_prop


  FROM oral_questions
    LEFT JOIN question_topics ON oral_questions.question_id = question_topics.question_id
  
  GROUP BY 
    oral_questions.question_tabled_when
  ")


time_data$date_asked <- as.Date(time_data$date_asked)
time_data$year <- format(time_data$date_asked, "%Y") 

plot <- time_data %>%
  ggplot(aes(x=date_asked)) +
  geom_col(aes(y=econ_count)) + 
  geom_smooth(aes(y=econ_count))

plot

```

### Geography

```{r geog-plot, eval = TRUE}

geog_data <- dbGetQuery(db, 
  "
  SELECT 
    constituencies.cons_name AS constituency, 
    
    SUM(question_topics.is_econ) AS econ_count,
    SUM(question_topics.is_health_welf) AS health_welf_count, 
    COUNT(*) AS total_question_count,
    SUM(question_topics.is_econ)/COUNT(*) AS econ_prop,
    SUM(question_topics.is_health_welf)/COUNT(*) AS health_welf_prop,
    
    constituencies.shapefile AS con_shapefile


  FROM oral_questions
    JOIN question_topics 
      ON oral_questions.question_id = question_topics.question_id
    JOIN members
      ON oral_questions.member_asking_Mnis_ID = members.member_Mnis_ID
        AND oral_questions.question_tabled_when BETWEEN members.member_date_valid_min AND members.member_date_valid_max
    JOIN constituencies 
      ON members.member_latest_constituency = constituencies.constituency_id
  
  GROUP BY constituencies.cons_name
  "
  )

# Plot base map
tm_shape(geog_data$con_shapefile) +
  tm_sf()



```



## Exploratory analysis 

```{r define-forest-functions}

apply_random_forest <- function(
  df, 
  dv_name = "",
  output_name = "",
  seed = 123, 
  initial_split_prop = 0.8, 
  folds = 5, 
  tuning_levels = 4, 
  cores = parallel::detectCores()) {

    df <- df %>%
      rename(dv = all_of(dv_name))

    set.seed(seed)
    split <- initial_split(df, prop = initial_split_prop)

    train <- training(split)
    test <- testing(split)
    folds <- vfold_cv(train, v = folds)
    
    print(paste0("LOG | ", output_name, " | ", Sys.time(), " | Splits done." ))

    recipe <- recipe(dv ~ ., data = df)

    spec <- rand_forest(
      mtry = tune(), 
      trees = tune(), 
      min_n = tune()) %>%
    set_mode("classification") %>%
    set_engine("ranger", num.threads = cores, importance = "impurity")

  workflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(spec)
  
  tuning_grid <- grid_regular(
    mtry(range = c(1, ncol(df)-1)), # Change to number of independent variables
    trees(),
    min_n(),
    levels = tuning_levels
    )

  print(paste0("LOG | ", output_name, " | ", Sys.time(), " | Tuning started." ))
  set.seed(seed)
  tune <- workflow %>%
    tune_grid(
      resamples = folds, 
      grid = tuning_grid)

  saveRDS(tune, paste0("data/random-forest-outputs/", output_name, "_tune.Rds"))
  print(paste0("LOG | ", output_name, " | ", Sys.time(), " | Tuning done." ))
  
  final <- workflow %>%
    finalize_workflow(parameters = select_best(tune, "roc_auc"))

  set.seed(seed)

  last_fit <- final %>%
    last_fit(split = split)
  
  saveRDS(last_fit, paste0("data/random-forest-outputs/", output_name, "last_fit.Rds") )

  print(paste0("LOG | ", output_name, " | ", Sys.time(), " | Final output done :)" ))
  }

# Function for mean imputation

replace_na_with_mean <- function(x) {
  mean_value <- mean(x, na.rm = TRUE)
  ifelse(is.na(x), mean_value, x)
}


```

```{r econ-tree}


# Apply for econ

# FIXME: check why filter BETWEEN reduces number returned 
## I think it might be because there are observations *after* the last change 


analysis_df_econ <- dbGetQuery(
  db, 
  "
  SELECT 
    question_topics.is_econ AS is_econ,

    members.member_latest_party,
    members.member_gender,
    constituencies.last_election_1_electorate,   
    constituencies.last_election_1_turnout,         
    constituencies.last_election_1_majority,               
    constituencies.last_election_1_isGeneralElection,              
    constituencies.region_nation_hoclib23,           
    constituencies.population_hoclib23,          
    constituencies.area_hoclib23,                   
    constituencies.age_0_29_hoclib23,            
    constituencies.age_30_64_hoclib23,               
    constituencies.age_65_plus_hoclib23,         
    constituencies.uc_claimants_hoclib23,            
    constituencies.median_house_price_hoclib23 

  FROM oral_questions
  JOIN question_topics
    ON oral_questions.question_id = question_topics.question_id
  JOIN members 
    ON oral_questions.member_asking_Mnis_ID = members.member_Mnis_ID
      AND oral_questions.question_tabled_when BETWEEN members.member_date_valid_min AND members.member_date_valid_max
  JOIN constituencies 
    ON members.member_latest_constituency = constituencies.constituency_id

  "
) %>%
  mutate(is_econ = factor(
    is_econ, levels = c(0,1), labels = c(FALSE, TRUE))) 


# TODO: should I just exclude and focus on English and Welsh MPs?

# Check which columns are missing
#sapply(analysis_df_econ, function(x) any(is.na(x)))

# Apply the function to columns with NA
analysis_df_econ$median_house_price_hoclib23 <- replace_na_with_mean(analysis_df_econ$median_house_price_hoclib23)
analysis_df_econ$uc_claimants_hoclib23 <- replace_na_with_mean(analysis_df_econ$uc_claimants_hoclib23)

apply_random_forest(
  df = analysis_df_econ,
  dv_name = "is_econ",
  output_name = "econ",
  seed = 1145,
  initial_split_prop = 0.8,
  folds = 5,
  tuning_levels = 4,
  cores = parallel::detectCores())

```

```{r}
#econ_last_fit <- readRDS("data/rf_econ_last_fit.Rds"))

# Variable importance plot

econ_vi_plot <- econ_last_fit %>%
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10) + #  CANDO can vary this
  labs(title = "Variable importance for asking Economic Questions")

econ_vi_plot

```

# Code appendix
```{r, eval = TRUE}
sessionInfo()
```

```{r ref.label=knitr::all_labels(), echo=TRUE} 

```



