---
title: "MY472 Summative 2 Project"
author: "Candidate 30552"
format: 
    html:
        embed-resources: true 
---


```{r, include = FALSE} 
knitr::opts_chunk$set(
    echo = FALSE, 
    eval = FALSE,
    message = FALSE, 
    warning = FALSE)

```

```{r basic-setup, eval = TRUE}

# Can also be run by sourcing scripts/00_setup.R

# Define function to install or load packages
load_packages <- function(x) {
  y <- x %in% rownames(installed.packages())
  if(any(!y)) install.packages(x[!y])
  invisible(lapply(x, library, character.only=T))
  rm(x, y)
}

# Load required packagess
load_packages(c(
    "tidyverse",
    "here",
    # Database management
    "DBI",
    "RSQLite",
    # APIs and webscraping
    "httr",
    "RSelenium",
    # Text analysis 
    "tm", 
    # Geospatial plots
    "tmap", 
    "sf", 
    "ggrepel", 
    # Random forests
    "parallel",
    "ranger",
    "tidymodels",
    "vip", 
    "rpart", 
    "rpart.plot"
    ))

replace_null_with_na <- function(x) {
  if (is.list(x)) { # Checks for whether the item is a sublist 
    lapply(x, replace_null_with_na) # if it is, apply the function for each of the elements within the sublist
  } else { # If it isn't, simply apply the main function
    ifelse(is.null(x) || x == "null", "NA", x) 
  }
}

db_table_check <- function(database, table){
  rows <- dbGetQuery(database, paste0("SELECT COUNT(1) FROM ", table))
  cols <- dbListFields(database, table)
  cols_n <- length(cols)
  
  result = list(
    n_rows = rows[[1]],
    n_cols = cols_n,
    col_names = cols)
  return(result)
}


db <- DBI::dbConnect(RSQLite::SQLite(), here("data/parliament_database.sqlite"))

```

# Introduciton 

# Data 



To store the data I collect, I will use a local relational database. This will improve the efficiency of the storage because... It will also allow me to query only the variables I need for anlausis 

I draw on 4 endpoint from parlimanet's API to construct four three tables that I write out to database.

1. Oral questions 
2. Members
3. Constituencies 
4. Elections

Tables: 

1. oral_questions
2. mps
3. constituencies (containing election results)

I then will create an additional table with the results of the measurmeent I use to classify questions. 

4. question_topics 


## Parliament API 

First, I scraped all questions availble from the written and oral endpoints. Both endpoints only return up to 100 questions with one request. However, you can skip responses. Therefore, you can retrieve all questions by looping through... 

These reponses do not return the full text of each question, so I use these responses as a sampling frame 

```{r pull-oral-questions}

# This code can also be run by sourcing scripts/01_pull-oral-questions.R 

GET_qs <- function(endpoint_url, n_skip = 0) {
  url <- paste0(
    endpoint_url,
    "?parameters.skip=",
    n_skip,
    "&parameters.answeringDateStart=2023-01-01&parameters.answeringDateEnd=2023-12-31", # Limit to 2023
    "&parameters.take=100")

  response <-
    httr::GET(url) %>%
    httr::content("parsed") # Use :: because tm masks content 

  return(response)
}

# Define functions to pull all questions

pull_all_oral_qs <- function(endpoint_url){

  # Calculate how many questions are in the end point
  n_resp <- httr::GET(paste0(
    endpoint_url,
    "?parameters.answeringDateStart=2023-01-01&parameters.answeringDateEnd=2023-12-31", # Limit to 2023
    "&parameters.take=1")) %>%
    httr::content("parsed")
  n <- n_resp$PagingInfo$GlobalTotal

  # Questions can be pulled in batches of 100,
  # calculate how many time we will have to pull
  n_loops <- ceiling(n / 100)

  print(paste0("LOG | ", Sys.time(), " | Oral question pull starting"))

  for (i in 1:n_loops) {

    n_skip <- (i - 1) * 100 # Skip however many 100s the loop has run

    if (i == 1) { # On first iteration, make new list

      response <- GET_qs(endpoint_url, n_skip)
      response <- response$Response

    } else { # On all other iterations, append to existing list

      response_new <- GET_qs(endpoint_url, n_skip)
      response_new <- response_new$Response
      response <- c(response, response_new) # Merge responses

    }

    print(paste0("LOG | ", Sys.time(), " | ", i, " of ", n_loops, " done.")) # Print progress message
    Sys.sleep(1) # Sleep to avoid hammering the API

  }

  print(paste0("LOG | ", Sys.time(), " | Oral question pull done :)"))
  return(response)
}

## APPLY FUNCTIONS

oral_questions <- pull_all_oral_qs(
  "https://oralquestionsandmotions-api.parliament.uk/oralquestions/list")

saveRDS(oral_questions, "data/oral_questions_2023.RDS")

```


```{r clean-oral-questions, eval = TRUE}

oral_questions <- readRDS("data/oral_questions_2023.RDS")


for (i in seq_along(oral_questions)) { 
   # remove sublists, otherwise names do not match
    oral_questions[[i]]$AskingMember <- NULL
    oral_questions[[i]][["AnsweringMinister"]] <- NULL

  if (i == 1){
    question_df <- data.frame(lapply(
      oral_questions[[i]],
      function(x) ifelse(is.null(x), NA, x)))
  } else {
    question_df2 <- data.frame(lapply(
      oral_questions[[i]],
      function(x) ifelse(is.null(x), NA, x)))

    question_df <- rbind(question_df, question_df2)
  }
}

rm(question_df2, i)

### Clean dataframes and merge into one table ####

question_df <- question_df %>%
  select(
    question_id = Id,
    question_text = QuestionText,
    asking_member = AskingMemberId,
    question_tabled_when = TabledWhen,
    question_answering_when = AnsweringWhen,
    question_answering_body = AnsweringBody,
    question_answering_body_id = AnsweringBodyId,
    answering_member = AnsweringMinisterId)

```

```{r}
dbWriteTable(db, "oral_questions", question_df, overwrite = TRUE)
```

```{r}
db_table_check(db, "oral_questions")
```

Many MP characteristics change over time - such as the party they represent, the ministerial posts they hold, etc. Therefore, we have to construct a members table that accomodates these changes. The UK Parliament members API allows queries that specifiy a date. This will give us a unique response that is valid on the day of each question. I will write out this "summarised" version of the data to my database. Then in analysis, I can query the entry that is valid for when each question is asked 

```{r pull-members-endpoint}

# This code can also be run by sourcing scripts/02_pull-members-endpoint.R 

pull_members <- function(base_url, df) {

  for (i in seq_along(df$member_id)) {

    url <- paste0( # Build request URL
      base_url, "/",
      df$member_id[i],
      "?detailsForDate=",
      df$question_tabled_when[i])

    if (i == 1) { # If 1st iteration, create response,

      response <- httr::GET(url) %>% httr::content("parsed") # Pull request
      response <- response[1] # Extract list with response
      response <- c(
        date = df$question_tabled_when[i], response[[1]]) # Merge with date
      response <- list(response) # Convert to list

    } else { #  else create response2, then merge
      response_new <- httr::GET(url) %>% httr::content("parsed")
      response_new <- response_new[1]
      response_new <- c(
        date = df$question_tabled_when[i], response_new[[1]])
      response_new <- list(response_new)

      response <- c(response, response_new) # Merge responses
    }

    Sys.sleep(1)

    print(paste0("LOG Member Pull | ", Sys.time(), " | ", i, " of ", nrow(df), " done"))
  }
  return(response)
}

# Query question table to get MP-date pairs

members_asking <- dbGetQuery(db,
  "
  SELECT 
    asking_member AS member_id, 
    question_tabled_when
  FROM oral_questions
  ")

ministers_answering <- dbGetQuery(db,
  "
  SELECT 
    answering_member AS member_id,
    question_tabled_when
  FROM oral_questions
  ")

q_parameters <- rbind(members_asking, ministers_answering)

# Only keep unique MP-date pairs to avoid pulling the same information twice
q_parameters <- unique(q_parameters) %>%
  filter(member_id != 0) # Remove 0s because these indicate no minister has answered


q_parameters <- q_parameters %>%
  mutate( # Change format of dates to just YYYY-MM-DD
    question_tabled_when = str_extract(question_tabled_when, ".+?(?=T)"))

# Apply function to pull members 
members <- pull_members(
  "https://members-api.parliament.uk/api/Members",
  q_parameters)

saveRDS(members, "data/members_raw.Rds")
```

```{r clean-members-pull}
# from here
members <- readRDS("data/members_raw.Rds") 

# Replace "null" values with NA so they are kept in the structure of the list
members <-  lapply(
  members, 
  function(x) {lapply(x, replace_null_with_na)})

for (i in seq_along(members)) {

    if (i == 1) { 

      members_df <- members[i] %>%
        unlist() %>%
        t() %>%
        data.frame()

    } else {
      members_df_new <- members[i] %>%
        unlist() %>%
        t() %>%
        data.frame()

      members_df <- rbind(members_df, members_df_new)
    }

}

members_df <- members_df %>%
  select(
    member_date_valid = date, 
    member_id = id, 
    name_display = nameDisplayAs, 
    gender = gender, 
    latest_constituency = latestHouseMembership.membershipFromId,
    membership_start_date = latestHouseMembership.membershipStartDate, 
    membership_end_date = latestHouseMembership.membershipEndDate,
    membership_end_reason = latestHouseMembership.membershipEndReason,
    latest_party_id = latestParty.id,
    # Keep some party information 
    latest_party_name = latestParty.name,
    latest_party_ind = latestParty.isIndependentParty
  )

# CANDO add "policitcal interests" endpoint 

# Some MP characteristics change over time, so we collected unique MP-day queries. 
# However, characteristics do not change daily so there is lots of repitition.
# The following code groups MPs by the mutable variables (i.e. unique combinations, 
# then summarises the earliest valid, and the latest valid date)
# Before this function there are 4225 observations, and after, only 482.  

members_df <- members_df %>%
  group_by( # Group by all mutable variables apart from date
    member_id, 
    name_display,
    gender, 
    latest_constituency,
    membership_start_date,
    membership_end_date,  
    latest_party_id
  ) %>%
  summarize( # Summarise earliest date this is valid for and latest. This gives us a range of vlaues where this combination is duplicated 
    member_date_valid_min = min(member_date_valid), 
    member_date_valid_max = max(member_date_valid)
  )

# CHECK has this fixed reductions? If not, try extending the max date for the biggest remainign max to now, and the min to the start of the range? 

# unique(members_df$member_id) %>% length() # This returns 474, indicating there are changes 

```

```{r}

dbWriteTable(db, "members", members_df, overwrite = TRUE)

```


```{r pull-constituency-endpoints}
# This code can also be run by sourcing scripts/03_pull-constituency-endpoints.R 

MPs <- dbGetQuery(db,
  "
  SELECT * 
  FROM members 
  ")

constituencies <- MPs$latest_constituency %>% 
  unique()

constituencies <- 
  data.frame(
    constituency_id = constituencies
  ) %>%
  mutate(

    cons_name = NA, 
    cons_start_date = NA, 
    cons_end_date = NA,

    last_election_1_electorate = NA,
    last_election_1_turnout = NA,
    last_election_1_majority = NA, 
    last_election_1_result = NA, 
    last_election_1_winning_party = NA,
    last_election_1_election_ID = NA,
    last_election_1_electionDate = NA,
    last_election_1_isGeneralElection = NA,

    last_election_2_electorate = NA,
    last_election_2_turnout = NA,
    last_election_2_majority = NA, 
    last_election_2_result = NA, 
    last_election_2_winning_party = NA,
    last_election_2_election_ID = NA,
    last_election_2_electionDate = NA,
    last_election_2_isGeneralElection = NA,

    last_election_3_electorate = NA,
    last_election_3_turnout = NA,
    last_election_3_majority = NA, 
    last_election_3_result = NA, 
    last_election_3_winning_party = NA,
    last_election_3_election_ID = NA,
    last_election_3_electionDate = NA,
    last_election_3_isGeneralElection = NA,

    last_election_4_electorate = NA,
    last_election_4_turnout = NA,
    last_election_4_majority = NA, 
    last_election_4_result = NA, 
    last_election_4_winning_party = NA,
    last_election_4_election_ID = NA,
    last_election_4_electionDate = NA,
    last_election_4_isGeneralElection = NA,

    shapefile = NA
    )

### Pull basic details 

pull_const_info <- function(cons_id) {
  url <- paste0(
    "https://members-api.parliament.uk/api/Location/Constituency/",
    cons_id)

    basic_info <- httr::GET(url) %>%
    httr::content("parsed")

    return(basic_info)
}

for(i in seq_along(constituencies$constituency_id)) {
  response <- pull_const_info(constituencies$constituency_id[i])
  response <- response[[1]]

  constituencies$cons_name[i] <- response$name
  constituencies$cons_start_date[i] <- response$startDate
  constituencies$cons_end_date[i] <- ifelse(is.null(response$endDate), NA, response$endDate)

  Sys.sleep(0.5)
  print(paste0("LOG | Constituency API call - basic | ", Sys.time(), " | ", i, " of ", length(constituencies$constituency_id), " done"))

}

### Pull shape file

get_cons_shapefile <- function(cons_id) {
  url <- paste0(
    "https://members-api.parliament.uk/api/Location/Constituency/",
    cons_id,
    "/Geometry")

  shapefile <- httr::GET(url) %>%
    httr::content("parsed")

  return(shapefile)
}

for(i in seq_along(constituencies$constituency_id)) {
  response <- get_cons_shapefile(constituencies$constituency_id[i])
  response <- response[[1]]

  constituencies$shapefile[i] <- response

  Sys.sleep(0.5)
  print(paste0("LOG | Constituency API call - shapefile | ", Sys.time(), " | ", i, " of ", length(constituencies$constituency_id), " done"))
}

### Pull election results

get_cons_election_results <- function(cons_id) {
  url <- paste0(
    "https://members-api.parliament.uk/api/Location/Constituency/",
    cons_id,
    "/ElectionResults")

  results <- httr::GET(url) %>%
    httr::content("parsed")

  return(results)
}

for (i in seq_along(constituencies$constituency_id)) {
  response <- get_cons_election_results(constituencies$constituency_id[i])
  response <- response[[1]]

  response <- lapply(response, function(lst) {lapply(lst, replace_null_with_na)})

  constituencies$last_election_1_electorate[i] <- response[[1]]$electorate
  constituencies$last_election_1_turnout[i] <- response[[1]]$turnout  
  constituencies$last_election_1_majority[i] <- response[[1]]$majority
  constituencies$last_election_1_result[i] <- response[[1]]$result
  # If no winner recorded, skip this and assign NA
  if(length(response[[1]]$winningParty) > 1) { # When there is content in the winning party sublist, the length will be greater than 1
    constituencies$last_election_1_winning_party[i] <- response[[1]]$winningParty$id
  } else {
     constituencies$last_election_1_winning_party[i] <- NA
  }
  constituencies$last_election_1_election_ID[i] = response[[1]]$electionId
  constituencies$last_election_1_electionDate[i] = response[[1]]$electionDate
  constituencies$last_election_1_isGeneralElection[i] = response[[1]]$isGeneralElection

  constituencies$last_election_2_electorate[i] <- response[[2]]$electorate
  constituencies$last_election_2_turnout[i] <- response[[2]]$turnout 
  constituencies$last_election_2_majority[i] <- response[[2]]$majority
  constituencies$last_election_2_result[i] <- response[[2]]$result
  # If no winner recorded, skip this and assign NA
  if(length(response[[2]]$winningParty) > 1) {
    constituencies$last_election_2_winning_party[i] <- response[[2]]$winningParty$id
  } else {
     constituencies$last_election_2_winning_party[i] <- NA
  }
  constituencies$last_election_2_election_ID[i] = response[[2]]$electionId
  constituencies$last_election_2_electionDate[i] = response[[2]]$electionDate
  constituencies$last_election_2_isGeneralElection[i] = response[[2]]$isGeneralElection

  constituencies$last_election_3_electorate[i] <- response[[3]]$electorate
  constituencies$last_election_3_turnout[i] <- response[[3]]$turnout  
  constituencies$last_election_3_majority[i] <- response[[3]]$majority
  constituencies$last_election_3_result[i] <- response[[3]]$result
  # If no winner recorded, skip this and assign NA
  if(length(response[[3]]$winningParty) > 1) {
    constituencies$last_election_3_winning_party[i] <- response[[3]]$winningParty$id
  } else {
     constituencies$last_election_3_winning_party[i] <- NA
  }
  constituencies$last_election_3_election_ID[i] = response[[3]]$electionId
  constituencies$last_election_3_electionDate[i] = response[[3]]$electionDate
  constituencies$last_election_3_isGeneralElection[i] = response[[3]]$isGeneralElection

  constituencies$last_election_4_electorate[i] <- response[[4]]$electorate
  constituencies$last_election_4_turnout[i] <- response[[4]]$turnout   
  constituencies$last_election_4_majority[i] <- response[[4]]$majority
  constituencies$last_election_4_result[i] <- response[[4]]$result
  # If no winner recorded, skip this and assign NA
  if(length(response[[4]]$winningParty) > 1) {
    constituencies$last_election_4_winning_party[i] <- response[[4]]$winningParty$id
  } else {
     constituencies$last_election_4_winning_party[i] <- NA
  }
  constituencies$last_election_4_election_ID[i] = response[[4]]$electionId
  constituencies$last_election_4_electionDate[i] = response[[4]]$electionDate
  constituencies$last_election_4_isGeneralElection[i] = response[[4]]$isGeneralElection

  Sys.sleep(0.5)
  
  print(paste0("LOG | Constituency API call - elections | ", Sys.time(), " | ", i, " of ", length(constituencies$constituency_id), " done"))
}

saveRDS(constituencies, "data/constituencies_api_raw.Rds")
```

```{r selenium-scrape-hoc-dashboard}

# This code can also be run by sourcing scripts/04_selinium-scrape-HoC-dashboard.R 

# NOTE cons_hoc returns 610 not 472 because it was pulled based on constituencies in all oral questions, not just 2023

# Read in data from the constituency endpoint pull
cons <- readRDS("data/constituencies_api_raw.Rds") 

# Make new dataframe
cons <- cons %>%
  select(constituency_id, cons_name) %>%
  unique() %>% # Keep only unqiue constituencies 
  mutate( # Initialise variables
    region_nation_hoclib23 = NA,
    population_hoclib23 = NA,
    area_hoclib23 = NA,
    age_0_29_hoclib23 = NA,
    age_30_64_hoclib23 = NA,
    age_65_plus_hoclib23 = NA, 
    uc_claimants_hoclib23 = NA, 
    median_house_price_hoclib23 = NA
  )

# Set selinium browser
rD <- rsDriver(browser=c("firefox"), verbose = F, port = netstat::free_port(random = TRUE), chromever = NULL) 
driver <- rD$client

# Define a list of css selectors

selector_list <- list()

selector_list$search_dropdown <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[1]/transform/div/div[3]/div/div/visual-modern/div/div/div[2]/div/i"

selector_list$search_box <- "/html/body/div[7]/div[1]/div/div[1]/input"

selector_list$search_result <- "/html/body/div[7]/div[1]/div/div[2]/div/div[1]/div/div/div[1]/div/span"

selector_list$region_nation <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[2]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div"

selector_list$population <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[3]/transform/div/div[3]/div/div/visual-modern/div/div/div/p[2]/span"

selector_list$area <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[5]/transform/div/div[3]/div/div/visual-modern/div/div/div/p[2]/span"

selector_list$age_0_29 <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[11]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div/div[1]"

selector_list$age_30_64 <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[13]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div/div[1]"

selector_list$age_65_plus <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[15]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div/div[1]"

selector_list$uc_claimants <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[28]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div[1]/div[1]"

selector_list$house_prices <- "/html/body/div[1]/report-embed/div/div[1]/div/div/div/div/exploration-container/div/div/docking-container/div/div/div/div/exploration-host/div/div/exploration/div/explore-canvas/div/div[2]/div/div[2]/div[2]/visual-container-repeat/visual-container[39]/transform/div/div[3]/div/div/visual-modern/div/div/div/div[1]/div/div/div/div/div[2]/div[1]"

constituency_dash_scraper <- function(
  constituency_name, 
  wait_base = 1 # Allows user to adjust wait lengths (e.g if running on a slow connection)
                # If you get a 'could not find element' error, try adjusting the wait time as the dashboard takes a while to load 
  ){
  # Find dropdown box and click on it 
  search_dropdown <- driver$findElement(using = "xpath", value = selector_list$search_dropdown)
  search_dropdown$clickElement()
  
  # Find search box and type constituency name
  Sys.sleep(wait_base * 2)
  search_box <- driver$findElement(using = "xpath", value = selector_list$search_box)
  #search_box$clickElement() # Do not strictly need this, but if not working try uncommenting
  search_box$clearElement()
  search_box$sendKeysToElement(list(constituency_name))

  Sys.sleep(wait_base * 4) # This requires a long time to load.
  # Click on the first result to load data
  first_result <- driver$findElement(using = "xpath", value = selector_list$search_result)
  first_result$clickElement()
  
  Sys.sleep(wait_base * 4) # Wait for data to load
  
  # EXTRACT TEXT FROM ELEMENTS

  # Set defaults as NA
  region_nation_text <- NA
  population_text <- NA
  area_text <- NA
  age_0_29_text <- NA
  age_30_64_text <- NA
  age_65_plus_text <- NA
  uc_claimants_text <- NA
  house_prices_text <- NA

  # Region or nation
  tryCatch({ # Prevent loop from closing if no data available
  suppressMessages({ 
    region_nation <- driver$findElement(using = "xpath", value = selector_list$region_nation)
    region_nation_text <- region_nation$getElementText()[[1]]
    })
  }, error = function(e) {
    # Print error message, no need to assign NA as we have set NA as default
    print(paste0("Log: NA assigned for REGION/NATION at iteration ", i))
  })

  # Population 
  tryCatch({
  suppressMessages({ 
    population <- driver$findElement(using = "xpath", value = selector_list$population)
    population_text <- population$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for POPULATION at iteration ", i))
  })

  # Area in sq km
  tryCatch({
  suppressMessages({ 
    area <- driver$findElement(using = "xpath", value = selector_list$area)
    area_text <- area$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for AREA at iteration ", i))
  })

  # Age composition 
  tryCatch({
  suppressMessages({
    age_0_29 <- driver$findElement(using = "xpath", value = selector_list$age_0_29)
    age_0_29_text <- age_0_29$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for AGE 0-29 PLUS at iteration ", i))
  })

  tryCatch({
  suppressMessages({
    age_30_64 <- driver$findElement(using = "xpath", value = selector_list$age_30_64)
    age_30_64_text <- age_30_64$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for AGE 30-64 PLUS at iteration ", i))
  })

  tryCatch({
  suppressMessages({
    age_65_plus <- driver$findElement(using = "xpath", value = selector_list$age_65_plus)
    age_65_plus_text <- age_65_plus$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for AGE 64 PLUS at iteration ", i))
  })

  # Universal credit claimants 
  tryCatch({
  suppressMessages({
    uc_claimants <- driver$findElement(using = "xpath", value = selector_list$uc_claimants)
    uc_claimants_text <- uc_claimants$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for UC CLAIMANTS at iteration ", i))
  })

  # House price
  tryCatch({
    suppressMessages({
      house_prices <- driver$findElement(using = "xpath", value = selector_list$house_prices)
      house_prices_text <- house_prices$getElementText()[[1]]
    })
  }, error = function(e) {
    print(paste0("Log: NA assigned for HOUSE PRICE at iteration ", i))
  })

 # Merge results into a list
  results = list(
    region_nation_text, 
    population_text, area_text, 
    age_0_29_text, age_30_64_text, age_65_plus_text,
    uc_claimants_text, house_prices_text)

  return(results)

}

# Run the scraper

# Navigate to home page outside of the loop to avoid reloading each time
driver$navigate("https://commonslibrary.parliament.uk/constituency-dashboard/")

Sys.sleep(1)

# The dashboard exists within a sub-page. Unless we "switch" to this subframe, the css paths will be broken
# Identify and switch to sub-page 
iframe <- driver$findElement(using = "xpath", value = "//iframe[@title='Constituency dashboard']")
driver$switchToFrame(iframe)
Sys.sleep(4)

# Set the number to start from in case loop is interuppted but we have cached results
start_from = 1 

for (i in start_from:length(cons$constituency_id)) {

  results <- constituency_dash_scraper(cons$cons_name[i], wait_base = 1)

    cons$region_nation_hoclib23[i] <- results[[1]]

    cons$population_hoclib23[i] <- results[[2]]

    cons$area_hoclib23[i] <- results[[3]]

    cons$age_0_29_hoclib23[i] <- results[[4]]
    cons$age_30_64_hoclib23[i] <- results[[5]]
    cons$age_65_plus_hoclib23[i] <- results[[6]]

    cons$uc_claimants_hoclib23[i] <- results[[7]]
    cons$median_house_price_hoclib23[i] <- results[[8]]

 # Cache results collected so far
  if(i == start_from){
    saveRDS(cons, paste0("data/cache_cons_at", i, ".Rds"))
  } else {
     saveRDS(cons, paste0("data/cache_cons_at", i, ".Rds"))
     file.remove(paste0("data/cache_cons_at", i-1, ".Rds")) # delete last cached object
  }
  
  Sys.sleep(1)

  print(paste0(i, " of ", nrow(cons), " done."))

}

# Kill driver and java processes
driver$close()
rD$server$stop()
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)

# Save output
saveRDS(cons, "data/hoc_library_scrape_raw.Rds")
```

```{r clean-constituency-data}

# Clean dashboard data

cons_hoc <- readRDS("data/hoc_library_scrape_raw.Rds")

# pop numeric 
cons_hoc$population_hoclib23 <- cons_hoc$population_hoclib23 %>%
  str_remove_all(",") %>%
  as.numeric()

# area numeric

cons_hoc$area_hoclib23 <- cons_hoc$area_hoclib23 %>%
  str_extract(".*(?=\\s*sq\\.\\s*km)") %>%
  str_remove_all(",") %>%
  as.numeric()

# age perc

cons_hoc$age_0_29_hoclib23 <- cons_hoc$age_0_29_hoclib23 %>%
  str_remove_all("%") %>%
  as.numeric()
cons_hoc$age_0_29_hoclib23 <- cons_hoc$age_0_29_hoclib23/100 # Convert to proportion 

cons_hoc$age_30_64_hoclib23 <- cons_hoc$age_30_64_hoclib23 %>%
  str_remove_all("%") %>%
  as.numeric()
cons_hoc$age_30_64_hoclib23 <- cons_hoc$age_30_64_hoclib23/100 # Convert to proportion 

cons_hoc$age_65_plus_hoclib23 <- cons_hoc$age_65_plus_hoclib23 %>%
  str_remove_all("%") %>%
  as.numeric()
cons_hoc$age_65_plus_hoclib23 <- cons_hoc$age_65_plus_hoclib23/100 # Convert to proportion 

# uc numeric

cons_hoc$uc_claimants_hoclib23 <- cons_hoc$uc_claimants_hoclib23 %>%
  str_remove_all(",") %>%
  as.numeric()

# house price numeric

cons_hoc$median_house_price_hoclib23 <- cons_hoc$median_house_price_hoclib23 %>%
  str_remove_all(",|Â£") %>%
  as.numeric()

# Merge API and dashboard data 

cons_api <- readRDS("data/constituencies_api_raw.Rds")

cons_hoc <- cons_hoc %>%
  select(-cons_name)

cons <- left_join(cons_api, cons_hoc, by = "constituency_id")

# Write out to database 

dbWriteTable(db, "constituencies", cons, overwrite = TRUE)

```


```{r}
dbListTables(db)
db_table_check(db, "oral_questions")
db_table_check(db, "members")
db_table_check(db, "constituencies")
```



# Analysis 

## Measure topics 

```{r}

# TODO add measure by dept 

question_text <- dbGetQuery(db, 
  "
  SELECT question_id, question_text
  FROM oral_questions
  "
)

# Measure 

# Initalise variables as NA
question_text$is_econ <- NA
question_text$is_health_welf <- NA

# clean question text 

question_text$question_text <- question_text$question_text %>%
    tolower() %>% # Convert to lower case
    tm::removePunctuation() # remove punctuation

# Define dictionaries 
# TODO: better define dictionaries

econ_dict <- 
  "econ*|inflat*|grow*|interest rate*|budget*|gdp|autumn statement*|financ*|chancellor"
health_welf_dict <- " nhs |universal credit| covid |pandemic*"

question_text <- question_text %>%
  mutate(
    is_econ = NA,
    is_health_welf = NA
  ) %>%
  mutate(
    is_econ = 
      ifelse(
        str_detect(question_text, econ_dict), 1, 0),
    is_health_welf = 
      ifelse(
        str_detect(question_text, health_welf_dict), 1, 0)
  )

dbWriteTable(db, "question_topics", question_text, overwrite = TRUE)

 ``` 


## Descriptive analysis 

### Time 

```{r time-plot, eval = TRUE}

# Query database to return a table with the count of questions asked on each day 

time_data <- dbGetQuery(db, 
  "
  SELECT 
    oral_questions.question_tabled_when AS date_asked,
    SUM(question_topics.is_econ) AS econ_count,
    SUM(question_topics.is_health_welf) AS health_welf_count, 
    COUNT(*) AS total_question_count,
    SUM(question_topics.is_econ)/COUNT(*) AS econ_prop,
    SUM(question_topics.is_health_welf)/COUNT(*) AS health_welf_prop


  FROM oral_questions
    LEFT JOIN question_topics ON oral_questions.question_id = question_topics.question_id
  
  GROUP BY 
    oral_questions.question_tabled_when
  ")


time_data$date_asked <- as.Date(time_data$date_asked)
time_data$year <- format(time_data$date_asked, "%Y") 

plot <- time_data %>% # TODO make time plot work, or plot by party 
  ggplot(aes(x=date_asked)) +
  geom_col(aes(y=econ_prop)) + 
  geom_col(aes(y=health_welf_prop), color = "red")

plot

```

### Party 

```{r}
party_data <- dbGetQuery(db, 
  "
  SELECT 
    members.latest_party_id,

    SUM(question_topics.is_econ) AS econ_count,
    SUM(question_topics.is_health_welf) AS health_welf_count, 
    COUNT(*) AS total_question_count,
    SUM(question_topics.is_econ)/COUNT(*) AS econ_prop,
    SUM(question_topics.is_health_welf)/COUNT(*) AS health_welf_prop 

    FROM oral_questions 
      LEFT JOIN question_topics ON oral_questions.question_id = question_topics.question_id
      LEFT JOIN members ON oral_questions.asking_member = members.member_id

    GROUP BY members.latest_party_id

  ")
```

### Geography

```{r geog-plot, eval = TRUE}

geog_data <- dbGetQuery(db, # FIXME: check why filter BETWEEN reduces number returned  
  "
  SELECT 
    constituencies.cons_name AS constituency, 
    
    SUM(question_topics.is_econ) AS econ_count,
    SUM(question_topics.is_health_welf) AS health_welf_count, 
    COUNT(*) AS total_question_count,
    SUM(question_topics.is_econ)/COUNT(*) AS econ_prop,
    SUM(question_topics.is_health_welf)/COUNT(*) AS health_welf_prop,
    
    constituencies.shapefile AS con_shapefile


  FROM oral_questions
    JOIN question_topics 
      ON oral_questions.question_id = question_topics.question_id
    JOIN members
      ON oral_questions.asking_member = members.member_id
        /* select row where date of question comes between the dates valid range */
        AND oral_questions.question_tabled_when BETWEEN members.member_date_valid_min AND members.member_date_valid_max 
    JOIN constituencies 
      ON members.latest_constituency = constituencies.constituency_id
  
  GROUP BY constituencies.cons_name
  "
  )

# Plot base map
tm_shape(geog_data$con_shapefile) + # FIXME map not working 
  tm_sf()

```



## Exploratory analysis 

```{r define-forest-functions}

apply_random_forest <- function(
  df, 
  dv_name = "",
  output_name = "",
  seed = 123, 
  initial_split_prop = 0.8, 
  folds = 5, 
  tuning_levels = 4, 
  cores = parallel::detectCores()) {

    df <- df %>%
      rename(dv = all_of(dv_name))

    set.seed(seed)
    split <- initial_split(df, prop = initial_split_prop)

    train <- training(split)
    test <- testing(split)
    folds <- vfold_cv(train, v = folds)
    
    print(paste0("LOG | ", output_name, " | ", Sys.time(), " | Splits done." ))

    recipe <- recipe(dv ~ ., data = df)

    spec <- rand_forest(
      mtry = tune(), 
      trees = tune(), 
      min_n = tune()) %>%
    set_mode("classification") %>%
    set_engine("ranger", num.threads = cores, importance = "impurity")

  workflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(spec)
  
  tuning_grid <- grid_regular(
    mtry(range = c(1, ncol(df)-1)), # Change to number of independent variables
    trees(),
    min_n(),
    levels = tuning_levels
    )

  print(paste0("LOG | ", output_name, " | ", Sys.time(), " | Tuning started." ))
  set.seed(seed)
  tune <- workflow %>%
    tune_grid(
      resamples = folds, 
      grid = tuning_grid)

  saveRDS(tune, paste0("data/random-forest-outputs/", output_name, "_tune.Rds"))
  print(paste0("LOG | ", output_name, " | ", Sys.time(), " | Tuning done." ))
  
  final <- workflow %>%
    finalize_workflow(parameters = select_best(tune, "roc_auc"))

  set.seed(seed)

  last_fit <- final %>%
    last_fit(split = split)
  
  saveRDS(last_fit, paste0("data/random-forest-outputs/", output_name, "last_fit.Rds") )

  print(paste0("LOG | ", output_name, " | ", Sys.time(), " | Final output done :)" ))
  }

# Function for mean imputation

replace_na_with_mean <- function(x) {
  mean_value <- mean(x, na.rm = TRUE)
  ifelse(is.na(x), mean_value, x)
}


```

```{r econ-tree}

# Apply for econ

# FIXME same with the BETWEEN filter
## I think it might be because there are observations *after* the last change - work back somehow?? replace all max dates for an ID with the last Q??
## Also, I should do the "unique" thing with only display name (after column selection), as that seems to be the main source of change

# TODO: add DVs 
# seat marginality measure - average majority in last four elections, count of party wins
# add average turnout measure 
# add average registration rate (electorate/population) over last 4
# add answering minister details (party, gender)
# add matches dummies (same party, same gender)
# add length of experience from membership start date 
# add is.independent from members 

db_table_check(db, "members")
db_table_check(db, "oral_questions")
db_table_check(db, "constituencies")
db_table_check(db, "question_topics")

analysis_df_econ <- dbGetQuery(
  db, 
  "
  SELECT 
    question_topics.is_econ AS is_econ, /* DV */
    
    /* IVs */ 

    members.latest_party_id AS asking_MP_party,
    members.gender AS asking_MP_gender,

    /* Majority */ 
    constituencies.last_election_1_majority, 
    constituencies.last_election_2_majority, 
    constituencies.last_election_3_majority, 
    constituencies.last_election_4_majority,

    /* turnout */ 
    constituencies.last_election_1_turnout,
    constituencies.last_election_2_turnout,
    constituencies.last_election_3_turnout,
    constituencies.last_election_4_turnout,

   /* registration rates */ 
    constituencies.last_election_1_electorate/population_hoclib23,
    constituencies.last_election_2_electorate/population_hoclib23,
    constituencies.last_election_3_electorate/population_hoclib23,
    constituencies.last_election_4_electorate/population_hoclib23

   /* minister answering for matches dummies */   # TODO work out how to do this from one table .. hmm

   /* registration rates */ 


  FROM oral_questions
  JOIN question_topics
    ON oral_questions.question_id = question_topics.question_id
  JOIN members 
    ON oral_questions.asking_member = members.member_id
      AND oral_questions.question_tabled_when BETWEEN members.member_date_valid_min AND members.member_date_valid_max /* FIXME this filter is broken */
  JOIN constituencies 
    ON members.latest_constituency = constituencies.constituency_id

  "
) %>%
  mutate(is_econ = factor(
    is_econ, levels = c(0,1), labels = c(FALSE, TRUE))) 

# TODO: take averages outside of SQL
## TODO: multiply majority by whether it's their majority or not 
# TODO: measure "matches" averages outside of SQL

# CANDO: should I just exclude and focus on English and Welsh MPs?

# Check which columns are missing
#sapply(analysis_df_econ, function(x) any(is.na(x)))

# Apply the function to columns with NA
analysis_df_econ$median_house_price_hoclib23 <- replace_na_with_mean(analysis_df_econ$median_house_price_hoclib23)
analysis_df_econ$uc_claimants_hoclib23 <- replace_na_with_mean(analysis_df_econ$uc_claimants_hoclib23)

apply_random_forest(
  df = analysis_df_econ,
  dv_name = "is_econ",
  output_name = "econ",
  seed = 1145,
  initial_split_prop = 0.8,
  folds = 5,
  tuning_levels = 4,
  cores = parallel::detectCores())

```

```{r}
#econ_last_fit <- readRDS("data/rf_econ_last_fit.Rds"))

# Variable importance plot

econ_vi_plot <- econ_last_fit %>%
  pluck(".workflow", 1) %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10) + #  CANDO can vary this
  labs(title = "Variable importance for asking Economic Questions")

econ_vi_plot

```


```{r, eval = TRUE}
# Disconnect from local database
DBI::dbDisconnect(db)
```

# Code appendix
```{r, eval = TRUE}
sessionInfo()
```

```{r ref.label=knitr::all_labels(), echo=TRUE} 

```



